# GrizzlyDuck Tests

This directory contains comprehensive tests for the GrizzlyDuck data manipulation library.

## Test Structure

```text
tests/
├── __init__.py              # Test package initialization
├── test_list.py            # Core List class functionality tests
├── test_joins.py           # Join functionality tests
├── test_sql_template.py    # SQL template rendering tests
├── test_integration.py     # Integration and workflow tests
├── test_utils.py          # Test utilities and helpers
├── run_tests.py           # Test runner script
└── README.md              # This file
```

## Test Categories

-### Unit Tests (`test_list.py`)

- Basic List class initialization and context management
- Mathematical operations (mean, multiply, quantile)
- Statistical functions (outlier detection, median of means)
- Data manipulation methods (filter, select, order, limit)
- Method chaining functionality
- Error handling

-### Join Tests (`test_joins.py`)

- All join types (INNER, LEFT, RIGHT, FULL OUTER, CROSS, SEMI, ANTI)
- Join conditions and USING clauses
- Multiple joins in single queries
- Join type enum conversion
- Error handling for invalid joins

-### SQL Template Tests (`test_sql_template.py`)

- Jinja2 template rendering
- All SQL clauses (SELECT, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT, OFFSET)
- Join clause generation
- Complex query combinations
- Whitespace and formatting

-### Integration Tests (`test_integration.py`)

- Real-world data analysis workflows
- Statistical analysis pipelines
- Complex filtering and aggregation
- Performance analysis scenarios
- Error recovery testing

## Running Tests

### Install Dependencies

```bash
pip install -r requirements.txt
```

### Run All Tests

```bash
python tests/run_tests.py
```

### Run Specific Test Types

```bash
python tests/run_tests.py --type unit        # Unit tests only
python tests/run_tests.py --type integration # Integration tests only
python tests/run_tests.py --type fast        # Exclude slow tests
```

### Run with Coverage

```bash
python tests/run_tests.py --coverage
```

### Using pytest Directly

```bash
pytest tests/ -v                    # All tests with verbose output
pytest tests/test_list.py -v        # Specific test file
pytest -m "not slow" tests/         # Exclude slow tests
pytest -k "test_mean" tests/        # Tests matching pattern
```

## Test Data

Tests use a combination of:

- Synthetic data generated by `DataGenerator` class
- Small samples of real datasets (like Titanic from seaborn)
- Predefined test datasets in `MockDataSets`

## Test Utilities

The `test_utils.py` file provides:

-### DataGenerator

- `create_employee_data()` - Employee dataset with demographics
- `create_sales_data()` - Sales transaction data
- `create_numeric_sequence()` - Simple numeric sequences
- `create_data_with_nulls()` - Data with missing values
- `create_outlier_data()` - Data with known outliers

-### TestAssertions

- `assert_dataframe_equals_ignoring_order()` - Order-independent DataFrame comparison
- `assert_columns_present()` - Column existence validation
- `assert_statistical_value_close()` - Statistical value comparison with tolerance
- `assert_sql_query_contains()` - SQL query validation

-### ListTestHelpers

- `create_test_list_with_data()` - List instance creation
- `perform_basic_operations()` - Standard operation testing
- `test_method_chaining()` - Dynamic method chaining tests
- `validate_list_state()` - List state validation

## Test Configuration

Tests are configured via `pytest.ini`:

- Test discovery patterns
- Output formatting
- Custom markers for test categorization
- Warning suppression

## Example Test Usage

```python
def test_custom_workflow():
    # Create test data
    data = DataGenerator.create_employee_data(100)
    
    # Test workflow
    with List(data) as lst:
        result = lst.filter("age >= 30") \
                   .select(['name', 'salary']) \
                   .order(['salary DESC']) \
                   .limit(10) \
                   .data()
        
        # Validate results
        assert len(result) <= 10
        TestAssertions.assert_columns_present(result, ['name', 'salary'])
        assert result['salary'].is_monotonic_decreasing
```

## Continuous Testing

For development, you can run tests continuously:

```bash
    pytest tests/ --watch  # If pytest-watch is installed
```

Or use your IDE's test runner for immediate feedback.

## Coverage Reports

HTML coverage reports are generated in `htmlcov/` directory when using `--coverage` option. Open `htmlcov/index.html` in your browser to view detailed coverage information.

## Adding New Tests

When adding new functionality to GrizzlyDuck:

1. Add unit tests to the appropriate `test_*.py` file
2. Add integration tests if the feature involves multiple components
3. Use test utilities for data generation and assertions
4. Add appropriate pytest markers for test categorization
5. Update this README if adding new test categories or utilities

## Debugging Failed Tests

- Use `pytest -v` for detailed test output
- Use `pytest --tb=long` for full tracebacks
- Use `pytest -s` to see print statements
- Use `pytest --pdb` to drop into debugger on failures
